{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ДЗ для работы с файлами\n",
    "\n",
    "у нас есть текст, которых лежит в файле city_smells.txt. Давайте проведем с ним элементарные количественные исследования: можно, например, узнать, сколько в тексте уникальных слов, размер самого длинного предложения и тд. Чтобы работать с текстом, который лежит в файле, нам надо:\n",
    "\n",
    "Открыть файл (не забудьте о режиме открытия и энкодинге)\n",
    "Прочитать его\n",
    "Сохранить текст в переменную, с которой можно работать дальше\n",
    "Предобработка текста: удалить пунктуацию, свести текст к нижнему регистру\n",
    "\n",
    "Что можно сделать с текстом: (пункты на выбор, минимум один)\n",
    "\n",
    "1. определить среднюю длину слова в тексте\n",
    "2. определить среднюю длину предложения в тексте\n",
    "3. посчитать, во сколько раз самое длинное предложение длиннее самого короткого (такое же можно сделать со словами)\n",
    "4. (не убирая пунктуацию) - среднее количество пунктуационных знаков в предложении\n",
    "5. количество уникальных слов и пророрция общему количеству слов в тексте\n",
    "6. что-то еще, что Вы сами захотите поисследовать\n",
    "Запишите результат Вашего мини-исследования в новый файл, добавьте его и (отдельный) файл с кодом в Ваш репозиторий."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# В Википедии приведено следующее определение предложения: \"Предложе́ние (в языке) — это единица языка, которая представляет \n",
    "# собой грамматически организованное соединение слов (или слово), обладающее смысловой и интонационной законченностью.[1] С\n",
    "# точки зрения пунктуации, предложение как законченная единица речи оформляется в конце точкой, восклицательным или \n",
    "# вопросительным знаками — или многоточием.\" В связи с этим я не учитывала слово \"источники\" и все, что идет после него, как\n",
    "# предложения и удалила эту часть.\n",
    "# Что касается пунктуационных знаков, то я исходила из того, что \"<...> в русском языке встречаются такие знаки пунктуации:\n",
    "# точка, многоточие, запятая, точка с запятой, скобки и кавычки, двоеточие, тире, восклицательный и вопросительный знаки\", т. \n",
    "# е., например, косую черту и дефис в качестве знаков пунктуации я не учитывала.\n",
    "import os, string, re\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "morph = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. Определить среднее количество пунктуационных знаков в предложении\n",
    "def determine_average_sentence_punctuation_marks_number(sentence_list):\n",
    "    punctuation_marks_number_list = []\n",
    "    sum = 0\n",
    "    for sentence in sentence_list:\n",
    "        punctuation_marks = re.findall(r'[.,;()«»:—!?]', sentence)\n",
    "        punctuation_marks_number = len(punctuation_marks)\n",
    "        punctuation_marks_number_list.append(punctuation_marks_number)\n",
    "    for number in punctuation_marks_number_list:\n",
    "        sum += number\n",
    "    average_punctuation_marks_number = round(sum / len(punctuation_marks_number_list), 1)\n",
    "    return 'Среднее количество пунктуационных знаков в предложении: ' + str(average_punctuation_marks_number)\n",
    "\n",
    "\n",
    "# 2. Определить среднюю длину предложения в тексте\n",
    "def determine_average_sentence_length(sentence_list):\n",
    "    sentence_length_list = []\n",
    "    sum = 0\n",
    "    for sentence in sentence_list:\n",
    "        sentence_length_list.append(len(sentence))\n",
    "        sum += len(sentence)\n",
    "    average_sentence_length = round(sum / len(sentence_length_list), 1)\n",
    "    return 'Средняя длина предложения (среднее количество символов с учетом пробелов): ' + str(average_sentence_length)\n",
    "\n",
    "\n",
    "# 3. Посчитать, во сколько раз самое длинное предложение длиннее самого короткого\n",
    "def determine_sentence_length_difference(sentence_list):\n",
    "    sentence_length_list = []\n",
    "    max_length = 0\n",
    "    for sentence in sentence_list:\n",
    "        sentence_length_list.append(len(sentence))\n",
    "    min_length = sentence_length_list[0]\n",
    "    for number in sentence_length_list:\n",
    "        if number > max_length:\n",
    "            max_length = number\n",
    "        if number < min_length:\n",
    "            min_length = number\n",
    "    difference = round(max_length / min_length, 1)\n",
    "    return 'Самое длинное предложение длиннее самого короткого в ' + str(difference) + ' раз(-а)'\n",
    "    \n",
    "\n",
    "# 5. Определить количество уникальных слов и пропорцию общему количеству слов в тексте\n",
    "def determine_unique_words_number_and_proproption(text):\n",
    "    lemma_list = []\n",
    "    text = preprocess_text(text)\n",
    "    word_list = text.lower().split()\n",
    "    for elem in word_list:\n",
    "        lemma_list.append(morph.parse(elem)[0].normal_form)\n",
    "    # Наличие небольшого количества неверных лемм (напр., \"свежевыпечь\") в итоге не сказывается на качестве данной функции.\n",
    "    lemma_list = list(set(lemma_list))\n",
    "    unique_words_percentage = round((len(lemma_list) / len(word_list)) * 100)\n",
    "    return ('Количество уникальных слов в тексте: ' + str(len(lemma_list)) + ', или ' + str(unique_words_percentage)\n",
    "    + '% от общего количества слов (' + str(len(word_list)) + ' без учета метаданных)')\n",
    "\n",
    "    \n",
    "# 1. Определить среднюю длину слова в тексте\n",
    "def determine_average_word_length(text):\n",
    "    text = preprocess_text(text)\n",
    "    word_list = text.lower().split()\n",
    "    sum = 0\n",
    "    for word in word_list:\n",
    "        sum += len(word)\n",
    "    average_word_length = round(sum / len(word_list), 1)\n",
    "    return 'Средняя длина слова в тексте: ' + str(average_word_length) + ' символа (-ов)' \n",
    "\n",
    "\n",
    "# От себя: определить ключевые слова в тексте для примерного понимания его тематики\n",
    "def find_key_words(text):\n",
    "    text = preprocess_text(text)\n",
    "    tokens = word_tokenize(text, language='russian')\n",
    "    tagged = pos_tag(tokens, lang='rus')\n",
    "    deletion_list = ['PR', 'CONJ', 'PART', 'INTJ', 'PRAEDIC-PRO', 'ADV-PRO', 'A-PRO=m', 'A-PRO=f', 'A-PRO=pl', 'S-PRO', \\\n",
    "                     'PARENTH', 'PRAEDIC', 'ADV', 'NUM=acc']\n",
    "    cleared_list = []\n",
    "    keywords_list = []\n",
    "    stops = stopwords.words('russian')\n",
    "    stops.append('есть')\n",
    "    [cleared_list.append(tag) for tag in tagged if tag[1] not in deletion_list]\n",
    "    words_normalized = [morph.parse(elem[0])[0].normal_form for elem in cleared_list]\n",
    "    # Из-за неправильной лемматизации слова \"данные\" в ключевые слова попадает слово \"дать\". В программе оно заменяется на  \n",
    "    # правильную лемму, чтобы не портить выдачу.\n",
    "    for index, elem in enumerate(words_normalized):\n",
    "        if elem == 'дать':\n",
    "            words_normalized[index] = 'данные'        \n",
    "    words_normalized = [word for word in words_normalized if word not in stops]\n",
    "    for elem in words_normalized:\n",
    "        if words_normalized.count(elem) >= 3:\n",
    "            keywords_list.append(elem)\n",
    "    keywords_list = list(set(keywords_list))\n",
    "    return 'Ключевые слова: ' + ', '.join(keywords_list)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Цифры не учитываются как слова.\n",
    "    return re.sub(r'\\d+', '', re.sub(r'[^\\w\\s-]', '', re.sub(r'/', ' ', text.lower())))\n",
    "     \n",
    "\n",
    "with open('Files\\\\city_smells.txt', 'r', encoding='utf-8') as input_text:\n",
    "    read_input_text = input_text.read()\n",
    "    \n",
    "remove_index = read_input_text.index('\\nИсточники:')\n",
    "remade_text = read_input_text[:(remove_index - 1)]\n",
    "remade_text = re.sub(r'\\n+', ' ', remade_text)\n",
    "sentence_list = re.split(r'(?:(?<=[.?!:])|(?<=$))\\s(?:(?=[А-Я])|(?=\\s))', remade_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32 30\n",
      "['Цвета выбраны не просто так. Исследователи нашли связи между запахами и цветами, преобладающими на анализируемых фотографиях.', 'На странице проекта, путешествуя по интерактивным картам, можно посмотреть соотношение запахов по каждому сегменту, например, в Барселоне на улице Бальмес запах еды преобладает: Вместе запахи складываются в комплексный и сложный ландшафт, уникальный для каждого города, и он нуждается в исследовании так же, как и ландшафт визуальный или звуковой.']\n"
     ]
    }
   ],
   "source": [
    "# Сравнение работы моего разделителя по предложениям и sent_tokenize из nltk:\n",
    "sent_tokenize_result = sent_tokenize(remade_text, 'russian')\n",
    "print(len(sentence_list), len(sent_tokenize_result))\n",
    "difference = list(set(sent_tokenize_result) - set(sentence_list))\n",
    "print(difference)\n",
    "# Поскольку мой вариант работает лучше для этого текста, я использую его, а не sent_tokenize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Files\\\\lesson_6_results.txt', 'w', encoding = 'utf-8')\n",
    "f.write(determine_average_sentence_punctuation_marks_number(sentence_list) + os.linesep)\n",
    "f.write(determine_average_sentence_length(sentence_list) + os.linesep)\n",
    "f.write(determine_sentence_length_difference(sentence_list) + os.linesep)\n",
    "f.write(determine_unique_words_number_and_proproption(remade_text) + os.linesep)\n",
    "f.write(determine_average_word_length(remade_text) + os.linesep)\n",
    "f.write(find_key_words(remade_text) + os.linesep)\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
