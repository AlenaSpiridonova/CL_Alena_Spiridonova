{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Vector-based (1)(1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8V8-FvacDXb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM_n97ehFGiM"
      },
      "source": [
        "# Семантический поиск с применением векторизации слов в помощь переводчику"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMXttCJ7cDXf"
      },
      "source": [
        "# Тренировка word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "orxPdRXQ894a",
        "outputId": "d5600db6-5ae4-4a48-8201-0d07bb358bce"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTenUyYDP38-"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import en_core_web_sm\n",
        "nlp = en_core_web_sm.load()\n",
        "from spacy.lang.en import English\n",
        "\n",
        "\n",
        "nlp = English()\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
        "\n",
        "# функция для разбиения текста на предложения\n",
        "def split_in_sentences(text):\n",
        "    doc = nlp(text)\n",
        "    return [str(sent).strip() for sent in doc.sents]\n",
        "\n",
        "\n",
        "# функция для частичной (т. к. исходный текст очень большой) предобработки текста,\n",
        "# заключающейся в удалении повторяющихся названий разделов и нумерации и \n",
        "# склеивании/разделении неправильно разделенных предложений\n",
        "def preprocess(line):\n",
        "  processed = []\n",
        "  sent = []\n",
        "  line = re.sub(r'(?<=\\W)\\”', '', re.sub(r'^\\“', '', line))\n",
        "  line = re.sub(r'^\\d*?\\.\\s*(?=[A-Za-z])', '', line)\n",
        "  line = re.sub(r'^\\d*?\\.\\d*?\\.\\s*(?=[A-Z])', '', line)\n",
        "  line = re.sub(r'^[A-Z]*?\\.\\s*(?=[A-Z])', '', line)\n",
        "  line = re.sub(r'^\\([a-z]\\)\\s*(?=[a-zA-Z])', '', line)\n",
        "  line = re.sub(r'^\\d*\\([a-z]\\)\\s*(?=[a-zA-Z])', '', line)\n",
        "  line = re.sub(r'^<...>\\s*', '', line)\n",
        "  line = line.strip('\\n')\n",
        "  if (line == 'THE CIRCUMSTANCES OF THE CASE' or line == 'PROCEDURE' or \n",
        "    line == 'THE FACTS' or line == 'RELEVANT DOMESTIC LAW AND PRACTICE' or \n",
        "    line == 'THE LAW' or line == 'Admissibility' or line == 'Merits' or \n",
        "    line == 'APPLICATION OF ARTICLE 41 OF THE CONVENTION' or \n",
        "    line == 'Damage' or line == 'Costs and expenses' or \n",
        "    line == 'FOR THESE REASONS, THE COURT UNANIMOUSLY' or \n",
        "    line == '' or \n",
        "    all(j.isdigit() or j in string.punctuation for j in line)):\n",
        "    return []\n",
        "\n",
        "  sent = split_in_sentences(line)\n",
        "  corrected = []\n",
        "  for i in sent:\n",
        "    if i == '\\\\n':\n",
        "      continue\n",
        "    elif re.findall(r'\\w\\.\\s(?=An|The|On|In)', i):\n",
        "      i = re.split(r'(?<=[.])\\s*(?=[A-Za-z])', i)\n",
        "      for sen in i:\n",
        "        corrected.append(sen)\n",
        "    else:\n",
        "      corrected.append(i)\n",
        "\n",
        "  previous_cor = ''\n",
        "  for cor in corrected:\n",
        "    if len(cor) <= 4:\n",
        "      continue\n",
        "    if (previous_cor != '' and cor != '\\n' and ((previous_cor[-1] == ',') \n",
        "    or (previous_cor[-1] == '.' and (cor[0].islower())) \n",
        "    or (previous_cor[-4:] == ' no.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == ' nos.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == '(no.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == '(nos.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-5:] == ' nos.' and cor[0].isdigit())\n",
        "    or (previous_cor[-4:] == ' No.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == ' Nos.' and cor[0].isdigit())\n",
        "    or (previous_cor[-4:] == ' NO.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == ' NOS.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-3:] == ' p.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-4:] == ' pp.' and cor[0].isdigit()) \n",
        "    or (previous_cor[-3:] == ' v.') \n",
        "    or (previous_cor[-1] == '§') \n",
        "    or (previous_cor[-4:] == 'Cap.') \n",
        "    or (previous_cor[-5:] == ' Doc.') \n",
        "    or (previous_cor[-1] == '['))):\n",
        "      res = previous_cor + ' ' + cor\n",
        "      processed[len(processed)-1] = res\n",
        "      previous_cor = res\n",
        "    else:\n",
        "      processed.append(cor)\n",
        "      previous_cor = cor\n",
        "  return processed\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wQfRm57vnNu"
      },
      "source": [
        "# считываем файл в формате json из датасета и записываем предложения построчно в\n",
        "# новый файл в текстовом формате\n",
        "with open('/content/drive/MyDrive/cases.json',) as f:\n",
        "  with open('preprocessed.txt', 'a') as record_file:\n",
        "    while True:\n",
        "      line = f.readline()\n",
        "      if not line:\n",
        "        break\n",
        "      line = line.strip()\n",
        "\n",
        "      if line[:12] == '\"content\": \"' and line[-2:] == '\",':\n",
        "        json_line = '{' + line[:-1] + '}'\n",
        "        json_line_data = json.loads(json_line)\n",
        "        preproc_list = preprocess(json_line_data['content'])\n",
        "        for el in preproc_list:\n",
        "          record_file.write(el + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_NojQxifFFe"
      },
      "source": [
        "import nltk\n",
        "import json\n",
        "\n",
        "# nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# считываем предложения из файла preprocessed.txt, токенизируем их и записываем\n",
        "# их в файл tokenized, чтобы не хранить в оперативной памяти  \n",
        "with open('/content/drive/MyDrive/preprocessed.txt') as file:\n",
        "  sentences = file.readlines()\n",
        "\n",
        "tokenized = []\n",
        "for sent in sentences:\n",
        "  sent = sent.rstrip('\\n')\n",
        "  tokenized.append(word_tokenize(sent))\n",
        "\n",
        "with open('tokenized.txt', 'a') as tok:\n",
        "  for i in tokenized:\n",
        "    i = json.dumps(i)\n",
        "    tok.write(i + '\\n')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I9ntorz0q0u"
      },
      "source": [
        "import json\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "\n",
        "# считываем предложения из файла tokenized.txt и подаем их модели для \n",
        "# построения векторов\n",
        "tokens = []\n",
        "with open('/content/drive/MyDrive/tokenized.txt', 'r') as tok:\n",
        "  while True:\n",
        "      sent = tok.readline()\n",
        "      if not sent:\n",
        "        break\n",
        "      sent = json.loads(sent)\n",
        "      tokens.append(sent)\n",
        "\n",
        "# обучаем модель и сохраняем ее\n",
        "model = Word2Vec(sentences=tokens, size=300, window=5, min_count=2, workers=10, iter=2, sg=1, negative=1)\n",
        "model.save(\"word2vec.model\")\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yev0eMfg39pm"
      },
      "source": [
        "# сохраняем полученные вектора\n",
        "word_vectors = model.wv\n",
        "word_vectors.save('word2vec.kv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpW62LTN4KU-"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "\n",
        "# загружаем вектора обученной модели\n",
        "model_vectors = KeyedVectors.load('word2vec.kv')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkZ17jVZYIcc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98929ec6-7643-459f-9895-bfac54dc5950"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "\n",
        "index2word_set = set(model_vectors.index2word)\n",
        "\n",
        "# функция для вычисления среднего вектора предложения\n",
        "def avg_feature_vector(words, model, num_features, index2word_set):\n",
        "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec\n",
        "\n",
        "# т. к. векторы всех предложений не удалось сохранить/правильно считать в виде \n",
        "# numpy массива по неизвестной причине, то для демонстрации работы программы \n",
        "# взят и векторизован объемный текст постановления \n",
        "# \"CASE OF KHODORKOVSKIY AND LEBEDEV v. RUSSIA\", входящего в состав исходного \n",
        "# датасета\n",
        "tokenized_sens = []\n",
        "with open('/content/drive/MyDrive/khodor_tokenized.txt', 'r') as tok:\n",
        "  while True:\n",
        "      sent = tok.readline()\n",
        "      if not sent:\n",
        "        break\n",
        "      sent = json.loads(sent)\n",
        "      tokenized_sens.append(sent)\n",
        "\n",
        "text_processed = []\n",
        "with open('/content/drive/MyDrive/khodor_processed.txt', 'r') as processed:\n",
        "   text_processed = processed.readlines()\n",
        "\n",
        "# вводится искомая фраза, переведенная на английский язык с помощью машинного \n",
        "# перевода, после чего выдаются три наиболее близких результата поиска,\n",
        "# полученыые путем сравнения косинусного сходства усредненных векторов предложений\n",
        "query = \"a fake deal\" \n",
        "query = word_tokenize(query)\n",
        "\n",
        "\n",
        "sim_list = []\n",
        "s1_afv = avg_feature_vector(query, model=model_vectors, num_features=300, index2word_set=index2word_set)\n",
        "\n",
        "\n",
        "i = 0\n",
        "for num, sent in enumerate(tokenized_sens):\n",
        "  emb = avg_feature_vector(sent, model=model_vectors, num_features=300, index2word_set=index2word_set)\n",
        "  sim = 1 - spatial.distance.cosine(s1_afv, emb)\n",
        "  if sim > 0.7:\n",
        "    result = sim, text_processed[i]\n",
        "    sim_list.append(result)\n",
        "  i += 1\n",
        "\n",
        "best_result = sorted(sim_list, reverse=True)[:3]\n",
        "for res in best_result:\n",
        "  print(res[0])\n",
        "  print(res[1])\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7079602479934692\n",
            "The merger was supposed to take place in two steps: firstly, completion of the deal on paper, and then unification of the new company’s management structures.\n",
            "\n",
            "0.7048969268798828\n",
            "A person cannot enter into a “sham” transaction by inadvertence; it is always a deliberate act.\n",
            "\n",
            "0.7037367820739746\n",
            "Even if it was true, it was perfectly normal for a businessman, for a number of reasons, to trade with the end-users not directly but through a corporate intermediary established by him.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnhSBlmWStpO"
      },
      "source": [
        "Сравнение результатов работы модели word2vec-google-news-300 по тому же запросу (с предложениями, найденными моделью, обученной только на корпусе судебных текстов)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ckc0pS5RmyLP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa819fe2-3625-418c-b868-7c8870bea142"
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader\n",
        "\n",
        "\n",
        "# для сравнения была использована модель, обученная на датасете Google\n",
        "google_model = gensim.downloader.load('word2vec-google-news-300')\n",
        "\n",
        "# word_vectors = google_model.wv\n",
        "google_model.wv.save(\"google2vec.wordvectors\")\n",
        "\n",
        "# wv = KeyedVectors.load(\"google2vec.wordvectors\", mmap='r')\n",
        "# google_model = wv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3AB758JDZ65"
      },
      "source": [
        "google_model = KeyedVectors.load(\"google2vec.wordvectors\", mmap='r')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CuR4CjfHnO2o",
        "outputId": "54737de3-ab51-4e9c-cd9f-674859d07e63"
      },
      "source": [
        "import numpy as np\n",
        "from scipy import spatial\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import json\n",
        "\n",
        "query = \"fake deal\"\n",
        "query = word_tokenize(query)\n",
        "\n",
        "index2word_set = set(google_model.index2word)\n",
        "# функция для вычисления среднего вектора предложения\n",
        "def avg_feature_vector(words, model, num_features, index2word_set):\n",
        "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec\n",
        "\n",
        "sentences = ['Even if it was true, it was perfectly normal for a businessman, for a number of reasons, to trade with the end-users not directly but through a corporate intermediary established by him.', \n",
        "             'A person cannot enter into a “sham” transaction by inadvertence; it is always a deliberate act.', \n",
        "             'The merger was supposed to take place in two steps: firstly, completion of the deal on paper, and then unification of the new company’s management structures.']\n",
        "sim_list = []\n",
        "s1_afv = avg_feature_vector(query, model=google_model, num_features=300, index2word_set=index2word_set)\n",
        "\n",
        "for sent in sentences:\n",
        "  s2_afv = avg_feature_vector(sent, model=google_model, num_features=300, index2word_set=index2word_set)\n",
        "  sim = 1 - spatial.distance.cosine(s1_afv, s2_afv)\n",
        "  print(sim, sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "0.0737365260720253 Even if it was true, it was perfectly normal for a businessman, for a number of reasons, to trade with the end-users not directly but through a corporate intermediary established by him.\n",
            "0.06405121833086014 A person cannot enter into a “sham” transaction by inadvertence; it is always a deliberate act.\n",
            "0.0636746808886528 The merger was supposed to take place in two steps: firstly, completion of the deal on paper, and then unification of the new company’s management structures.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F3sHSp3TOzh"
      },
      "source": [
        "Вывод: числовые показатели косинусного сходства у модели, обученной на судебных текстах, гораздо выше, чем у модели Google, обученной на наборе новостей."
      ]
    }
  ]
}